{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Natural Language Processing\n\n**Natural Language Processing (NLP)** is a field within Artificial Intelligence that focuses on facilitating machines' comprehension of human language in its natural form. Natural language can take the form of written text or spoken words, which humans use to communicate with one another. NLP aims to enable humans to interact with machines in a manner that feels intuitive and akin to everyday conversation.\n\n\n**Text Classification** is a process involved in Sentiment Analysis. It is classification of peoples opinion or expressions into different sentiments.","metadata":{}},{"cell_type":"markdown","source":"##  Importing Dependencies\n   We shall start by importing all the neccessary libraries. ","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport nltk \nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport re\nprint(\"Tensorflow Version\",tf.__version__)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2023-07-13T03:58:46.941858Z","iopub.execute_input":"2023-07-13T03:58:46.942425Z","iopub.status.idle":"2023-07-13T03:58:52.711144Z","shell.execute_reply.started":"2023-07-13T03:58:46.942363Z","shell.execute_reply":"2023-07-13T03:58:52.710168Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\nTensorflow Version 2.1.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#  Dataset Preprocessing\nIn this notebook, I am using **Sentiment-140** from [Kaggle](https://www.kaggle.com/kazanova/sentiment140). ","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/sentiment140/training.1600000.processed.noemoticon.csv',\n                 encoding = 'latin',header=None)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:58:52.713090Z","iopub.execute_input":"2023-07-13T03:58:52.713657Z","iopub.status.idle":"2023-07-13T03:58:56.383685Z","shell.execute_reply.started":"2023-07-13T03:58:52.713602Z","shell.execute_reply":"2023-07-13T03:58:56.379549Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-5c42c47c15bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m df = pd.read_csv('../input/sentiment140/training.1600000.processed.noemoticon.csv',\n\u001b[0;32m----> 2\u001b[0;31m                  encoding = 'latin',header=None)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2035\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2036\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2037\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2038\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n","\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."],"ename":"ParserError","evalue":"Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.","output_type":"error"}]},{"cell_type":"markdown","source":"We have to rename the columns ","metadata":{}},{"cell_type":"code","source":"df.columns = ['sentiment', 'id', 'date', 'query', 'user_id', 'text']\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:58:56.384641Z","iopub.status.idle":"2023-07-13T03:58:56.385304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We only need the sentiment and text columns in this project.","metadata":{}},{"cell_type":"code","source":"df = df[['sentiment','text']]","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:58:56.386670Z","iopub.status.idle":"2023-07-13T03:58:56.387571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentiment_dict = {0:\"Negative\", 4:\"Positive\"}\n\ndef label_decoder(label):\n  return sentiment_dict[label]","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:58:56.389100Z","iopub.status.idle":"2023-07-13T03:58:56.390059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.sentiment = df.sentiment.apply(lambda x: label_decoder(x))\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:58:56.391415Z","iopub.status.idle":"2023-07-13T03:58:56.392523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_count = df.sentiment.value_counts()\n\nplt.figure(figsize=(8,4))\nplt.bar(val_count.index, val_count.values)\nplt.title(\"Sentiment Data Distribution\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:58:56.394125Z","iopub.status.idle":"2023-07-13T03:58:56.395262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.sample(20)","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:58:56.403938Z","iopub.status.idle":"2023-07-13T03:58:56.404743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like we have a nasty data in text. Because in general we use lot of punctuations and other words without any contextual meaning. It have no value as feature to the model we are training. So we need to get rid of them.\n\n# Text Preprocessing\nTweet texts often consists of other user mentions, hyperlink texts, emoticons and punctuations. The texts need to be cleaned.\n","metadata":{}},{"cell_type":"markdown","source":"### Stemming/ Lematization\nFor grammatical reasons, documents are going to use different forms of a word, such as *write, writing and writes.* Additionally, there are families of derivationally related words with similar meanings. The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n\nStemming usually refers to a process that chops off the ends of words in the hope of achieving goal correctly most of the time and often includes the removal of derivational affixes. \n\nLemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base and dictionary form of a word\n![Stemming and Lematization](https://qph.fs.quoracdn.net/main-qimg-cd7f4bafaa42639deb999b1580bea69f)\n\n### Hyperlinks and Mentions\nTwitter is a social media platform where people can tag and mentions other people's ID and share videos and blogs from internet. So the tweets often contain lots of Hyperlinks and twitter mentions.\n\n- Twitter User Mentions - Eg. @arunrk7, @andrewng\n- Hyperlinks - Eg. https://keras.io, https://tensorflow.org\n\n### Stopwords\nStopwords are commonly used words in English which have no contextual meaning in an sentence. So therefore we remove them before classification. Some stopwords are...\n\n\n\n","metadata":{}},{"cell_type":"code","source":"stop_words = stopwords.words('english')\nstemmer = SnowballStemmer('english')\n\ntext_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:58:56.406370Z","iopub.status.idle":"2023-07-13T03:58:56.407243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(text, stem=False):\n  text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n  tokens = []\n  for token in text.split():\n    if token not in stop_words:\n      if stem:\n        tokens.append(stemmer.stem(token))\n      else:\n        tokens.append(token)\n  return \" \".join(tokens)","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:58:56.408769Z","iopub.status.idle":"2023-07-13T03:58:56.409664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.text = df.text.apply(lambda x: preprocess(x))","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:58:56.410981Z","iopub.status.idle":"2023-07-13T03:58:56.411834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_SEQUENCE_LENGTH = (df.text.apply(lambda x : len(x))).max()","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:58:56.413197Z","iopub.status.idle":"2023-07-13T03:58:56.414076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from wordcloud import WordCloud\n\nplt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 1000 , width = 1600 , height = 800).generate(\" \".join(df[df.sentiment == 'Positive'].text))\nplt.imshow(wc , interpolation = 'bilinear')","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:58:56.415443Z","iopub.status.idle":"2023-07-13T03:58:56.416361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Negative Words","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 1000 , width = 1600 , height = 800).generate(\" \".join(df[df.sentiment == 'Negative'].text))\nplt.imshow(wc , interpolation = 'bilinear')","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:58:56.417703Z","iopub.status.idle":"2023-07-13T03:58:56.418601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train and Test Split","metadata":{}},{"cell_type":"code","source":"TRAIN_SIZE = 0.8\nMAX_NB_WORDS = 100000\n# MAX_SEQUENCE_LENGTH = 30","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:58:56.419940Z","iopub.status.idle":"2023-07-13T03:58:56.420811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data, test_data = train_test_split(df, test_size=1-TRAIN_SIZE,\n                                         random_state=7) # Splits Dataset into Training and Testing set\nprint(\"Train Data size:\", len(train_data))\nprint(\"Test Data size\", len(test_data))","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:58:56.422168Z","iopub.status.idle":"2023-07-13T03:58:56.423072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenization\nIn TensorFlow's Keras, the tokenizer and pad sequencing functions are commonly used for text preprocessing in Natural Language Processing tasks, such as text classification or language generation. Let's discuss each function:\n\nTokenizer:\nThe tokenizer function in TensorFlow's Keras is responsible for converting text into a sequence of tokens, where each token represents a unique word or character. This process involves several steps:\n\na. Tokenization: The tokenizer splits the input text into individual words or characters, depending on the chosen configuration.\n\nb. Vocabulary Creation: It builds a vocabulary, which is essentially a mapping of unique tokens to integer values. Each word or character in the vocabulary is assigned a unique integer ID.\n\nc. Text Encoding: The tokenizer replaces each word or character in the input text with its corresponding integer ID from the vocabulary, thus transforming the text into a sequence of integers.\n\nThe tokenizer function provides additional functionalities like handling lowercase/uppercase, filtering out special characters, and controlling the maximum vocabulary size.\n\nPad Sequencing:\nPad sequencing is used to ensure that all input sequences have the same length, which is necessary for creating consistent input tensors for deep learning models. In NLP tasks, sequences often have varying lengths due to different sentence lengths in the text corpus.\nThe pad sequencing function in TensorFlow's Keras adds padding (typically zeroes) to the sequences that are shorter than the maximum sequence length. This ensures that all sequences have the same length, allowing them to be efficiently processed in batches. It involves the following steps:\n\na. Determining the maximum sequence length: The function calculates the maximum length among all input sequences.\n\nb. Padding: It adds zeroes (or any other specified padding value) to the shorter sequences until they reach the maximum sequence length.\n\nc. Creating fixed-length sequences: The function outputs sequences of equal length, ready to be used as input to the neural network.\n\nBy using the tokenizer and pad sequencing functions together, you can preprocess and transform raw text data into sequences of integers with consistent lengths, which can then be fed into deep learning models for training and inference.","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_data.text)\n\nword_index = tokenizer.word_index\nvocab_size = len(tokenizer.word_index) + 1\n\nprint(\"Vocabulary Size :\", vocab_size)","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:58:56.424402Z","iopub.status.idle":"2023-07-13T03:58:56.425309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we got a `tokenizer` object, which can be used to covert any word into a Key in dictionary (number).\n\nSince we are going to build a sequence model. We should feed in a sequence of numbers to it. And also we should ensure there is no variance in input shapes of sequences. It all should be of same lenght. But texts in tweets have different count of words in it. To avoid this, we seek a little help from `pad_sequence` to do our job. It will make all the sequence in one constant length `MAX_SEQUENCE_LENGTH`.","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\n\nx_train = pad_sequences(tokenizer.texts_to_sequences(train_data.text),\n                        maxlen = MAX_SEQUENCE_LENGTH)\nx_test = pad_sequences(tokenizer.texts_to_sequences(test_data.text),\n                       maxlen = MAX_SEQUENCE_LENGTH)\n\nprint(\"Training X Shape:\",x_train.shape)\nprint(\"Testing X Shape:\",x_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:58:56.426693Z","iopub.status.idle":"2023-07-13T03:58:56.427578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = train_data.sentiment.unique().tolist()","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:58:56.428915Z","iopub.status.idle":"2023-07-13T03:58:56.429811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Label Encoding \nWe are building the model to predict class in enocoded form (0 or 1 as this is a binary classification). We should encode our training labels to encodings.","metadata":{}},{"cell_type":"code","source":"encoder = LabelEncoder()\nencoder.fit(train_data.sentiment.to_list())\n\ny_train = encoder.transform(train_data.sentiment.to_list())\ny_test = encoder.transform(test_data.sentiment.to_list())\n\ny_train = y_train.reshape(-1,1)\ny_test = y_test.reshape(-1,1)\n\nprint(\"y_train shape:\", y_train.shape)\nprint(\"y_test shape:\", y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:58:56.431160Z","iopub.status.idle":"2023-07-13T03:58:56.432049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word Emdedding\nIn Language Model, words are represented in a way to intend more meaning and for learning the patterns and contextual meaning behind it. \n\n**Word Embedding** is one of the popular representation of document vocabulary.It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc.\n\nBasically, it's a feature vector representation of words which are used for other natural language processing applications.\n\nWe download the pre-trained embedding and use it in our model.\n\nThe pretrained Word Embedding like **GloVe & Word2Vec** gives more insights for a word which can be used for classification. \n\n\nIn this notebook, I use **GloVe Embedding from Stanford AI** which can be found [here](https://nlp.stanford.edu/projects/glove/)","metadata":{}},{"cell_type":"code","source":"!wget http://nlp.stanford.edu/data/glove.6B.zip\n!unzip glove.6B.zip","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:58:56.433363Z","iopub.status.idle":"2023-07-13T03:58:56.434241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GLOVE_EMB = '/kaggle/working/glove.6B.300d.txt'\nEMBEDDING_DIM = 300\nLR = 1e-3\nBATCH_SIZE = 512\nEPOCHS = 20\nMODEL_PATH = '.../output/kaggle/working/best_model.hdf5'","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:58:56.435574Z","iopub.status.idle":"2023-07-13T03:58:56.436467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings_index = {}\n\nf = open(GLOVE_EMB)\nfor line in f:\n  values = line.split()\n  word = value = values[0]\n  coefs = np.asarray(values[1:], dtype='float32')\n  embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' %len(embeddings_index))","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:58:56.437811Z","iopub.status.idle":"2023-07-13T03:58:56.438694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\nfor word, i in word_index.items():\n  embedding_vector = embeddings_index.get(word)\n  if embedding_vector is not None:\n    embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:58:56.440041Z","iopub.status.idle":"2023-07-13T03:58:56.440919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_layer = tf.keras.layers.Embedding(vocab_size,\n                                          EMBEDDING_DIM,\n                                          weights=[embedding_matrix],\n                                          input_length=MAX_SEQUENCE_LENGTH,\n                                          trainable=False)","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:58:56.442246Z","iopub.status.idle":"2023-07-13T03:58:56.443131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training - LSTM\nReccurent Neural Networks can handle a seqence of data and learn a pattern of input seqence to give either sequence or scalar value as output. In our case, the Neural Network outputs a scalar value prediction. \n\nFor model architecture, we use\n\n1) **Embedding Layer** - Generates Embedding Vector for each input sequence.\n\n2) **Conv1D Layer** - Its using to convolve data into smaller feature vectors. \n\n3) **LSTM** - Long Short Term Memory, its a variant of RNN which has memory state cell to learn the context of words which are at further along the text to carry contextual meaning rather than just neighbouring words as in case of RNN.\n\n4) **Dense** - Fully Connected Layers for classification\n","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Dropout\nfrom tensorflow.keras.layers import SpatialDropout1D\nfrom tensorflow.keras.callbacks import ModelCheckpoint","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:58:56.444466Z","iopub.status.idle":"2023-07-13T03:58:56.445354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n\nembedding_sequences = embedding_layer(sequence_input)\n\nx = SpatialDropout1D(0.2)(embedding_sequences)\nx = Conv1D(64, 5, activation='relu')(x)\nx = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2))(x)\nx = Dense(256, activation='relu')(x)\nx = Dropout(0.3)(x)\nx = Dense(256, activation='relu')(x)\noutputs = Dense(1, activation='sigmoid')(x)\nmodel = tf.keras.Model(sequence_input, outputs)","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:58:56.446680Z","iopub.status.idle":"2023-07-13T03:58:56.447576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Optimization Algorithm\nThis notebook uses Adam, optimization algorithm for Gradient Descent. You can learn more about Adam [here](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam)\n\n### Callbacks\nCallbacks are special functions which are called at the end of an epoch. We can use any functions to perform specific operation after each epoch. I used two callbacks here,\n\n- **LRScheduler** - It changes a Learning Rate at specfic epoch to achieve more improved result. In this notebook, the learning rate exponentionally decreases after remaining same for first 10 Epoch.\n\n- **ModelCheckPoint** - It saves best model while training based on some metrics. Here, it saves the model with minimum Validity Loss.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nmodel.compile(optimizer=Adam(learning_rate=LR), loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nReduceLROnPlateau = ReduceLROnPlateau(factor=0.1,\n                                     min_lr = 0.01,\n                                     monitor = 'val_loss',\n                                     verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:58:56.448928Z","iopub.status.idle":"2023-07-13T03:58:56.449813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Training on GPU...\") if tf.test.is_gpu_available() else print(\"Training on CPU...\")","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:58:56.451156Z","iopub.status.idle":"2023-07-13T03:58:56.452020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n                    validation_data=(x_test, y_test), callbacks=[ReduceLROnPlateau])","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:58:56.453345Z","iopub.status.idle":"2023-07-13T03:58:56.454220Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation\nNow that we have trained the model, we can evaluate its performance. We will use some evaluation metrics and techniques to test the model.\n\nLet's start with the Learning Curve of loss and accuracy of the model on each epoch.","metadata":{}},{"cell_type":"code","source":"s, (at, al) = plt.subplots(2,1, figsize = (15,8))\nat.plot(history.history['accuracy'], c= 'b')\nat.plot(history.history['val_accuracy'], c='r')\nat.set_title('model accuracy')\nat.set_ylabel('accuracy')\nat.set_xlabel('epoch')\nat.legend(['LSTM_train', 'LSTM_val'], loc='upper left')\n\nal.plot(history.history['loss'], c='m')\nal.plot(history.history['val_loss'], c='c')\nal.set_title('model loss')\nal.set_ylabel('loss')\nal.set_xlabel('epoch')\nal.legend(['train', 'val'], loc = 'upper left')\n\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:58:56.455551Z","iopub.status.idle":"2023-07-13T03:58:56.456447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model will output a prediction score between 0 and 1. We can classify two classes by defining a threshold value for it. In our case, I have set 0.5 as THRESHOLD value, if the score above it. Then it will be classified as **POSITIVE** sentiment.","metadata":{}},{"cell_type":"code","source":"def decode_sentiment(score):\n    return \"Positive\" if score>0.5 else \"Negative\"","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:58:56.457770Z","iopub.status.idle":"2023-07-13T03:58:56.458663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = model.predict(x_test, verbose=1, batch_size=5000)\ny_pred_1d = [decode_sentiment(score) for score in scores]","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:58:56.459992Z","iopub.status.idle":"2023-07-13T03:58:56.460882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Confusion Matrix\nConfusion Matrix provide a nice overlook at the model's performance in classification task","metadata":{}},{"cell_type":"code","source":"import itertools\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\ndef plot_confusion_matrix(cm, classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n\n    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=20)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, fontsize=13)\n    plt.yticks(tick_marks, classes, fontsize=13)\n\n    fmt = '.2f'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label', fontsize=17)\n    plt.xlabel('Predicted label', fontsize=17)","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:58:56.462235Z","iopub.status.idle":"2023-07-13T03:58:56.463131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnf_matrix = confusion_matrix(test_data.sentiment.to_list(), y_pred_1d)\nplt.figure(figsize=(6,6))\nplot_confusion_matrix(cnf_matrix, classes=test_data.sentiment.unique(), title=\"Confusion matrix\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:58:56.464467Z","iopub.status.idle":"2023-07-13T03:58:56.465332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Classification Scores","metadata":{}},{"cell_type":"code","source":"print(classification_report(list(test_data.sentiment), y_pred_1d))","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:58:56.466657Z","iopub.status.idle":"2023-07-13T03:58:56.467540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#You can learn more things from kaggle ","metadata":{}}]}